services:
  ollama:
    container_name: ollama
    hostname: ollama
    image: ollama/ollama # Official Ollama image
    volumes:
        - ~/.cache/ollama:/root/.cache/ollama  # Optional: Cache models on the host
        - ./pull_model.sh:/app/pull_model.sh
    # runtime: nvidia   # Enable to use the NVIDIA runtime and your Nvida GPU
    environment:
      - MODEL_NAME=${MODEL_NAME}
      # - NVIDIA_VISIBLE_DEVICES=all # Enable to use your Nvidia GPU
    entrypoint: /bin/bash -c "/app/pull_model.sh" 
    init: true

  chat-app:
    container_name: ollama-chat
    hostname: ollama-chat
    build: .  # Build from the current directory
    ports:
      - "5000:5000"
    environment:
      - OLLAMA_API_URL=${OLLAMA_API_URL}
      - MODEL_NAME=${MODEL_NAME}
    depends_on:
      - ollama